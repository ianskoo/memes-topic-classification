{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meme classification\n",
    "\n",
    "For this task, I will use the token counts method to vectorize the meme transcriptions. I am using the Scikit-Learn library for most of the next steps, and [this article](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) as reference.\n",
    "\n",
    "## Dataset splitting and model comparison before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['meme_id', 'meme_template', 'subreddit', 'category', 'url',\n",
       "       'transcription', 'title', 'tokenized_transcription', 'tokenized_title',\n",
       "       'tokenized_merged_transc_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"./data/final_dataset.csv\", index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"tokenized_transcription\"], df[\"category\"], random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596     ['foe', 'Poor', 'pe', 'ople', 'ee', 'eee', 'Le...\n",
       "5014                  ['nae', 'MELROSE', 'a', 'eS', 'aa']\n",
       "1612    ['BIRTH', 'CONTROL', 'EFFECTIVENESS', 'ED', 'a...\n",
       "2683    ['Thats', 'a', 'nasty', 'hoodie', 'Be', 'That'...\n",
       "652     ['Normal', 'girls', 'with', 'natural', 'skin',...\n",
       "                              ...                        \n",
       "3772    ['Who', 'beds', 'WAT', 'ae', 'A', 'Russian', '...\n",
       "5191    ['A', 'Joseph', 'Josephs', 'ae', 'A', 'A', 'wi...\n",
       "5226    ['Me', 'Can', 'I', 'nave', 'some', 'itriend', ...\n",
       "5390    ['Asking', 'your', 'crush', 'out', 'ad', 'a', ...\n",
       "860     ['When', 'you', 'get', 'a', 'bunch', 'of', 'do...\n",
       "Name: tokenized_transcription, Length: 4344, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a Naive Bayes classifier first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # Counts words per sentence\n",
    "                     ('tfidf', TfidfTransformer()), # Avoids giving more weight to longer sentences and reduces weight of very common words\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5405156537753223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's slightly better than random prediction. Let's now try using Support Vector Machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                   alpha=1e-3, random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7016574585635359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized sentence: ['Asmall', 'injury', 'like', 'stubbing', 'my', 'toe', 'that', 'goes', 'away', 'within', 'minutes', 'Me', 'Is', 'this', 'a', 'reason', 'to', 'stay', 'home', 'from', 'school', '?'], \n",
      "Predicted topic: Youth\n",
      "Actual topic: Youth\n"
     ]
    }
   ],
   "source": [
    "sample = X_test.sample(1)\n",
    "single_prediction = text_clf_svm.predict(sample)[0]\n",
    "sample_sentence = sample.values[0]\n",
    "print(f\"Sample tokenized sentence: {sample_sentence}, \\nPredicted topic: {single_prediction}\")\n",
    "print(f\"Actual topic: {y_test[sample.index].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Let's start by tuning the hyperparameters of the Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7265193370165746"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_gs = gs_clf.predict(X_test)\n",
    "np.mean(predicted_gs == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters of the Naive Bayes model, we can see an accuracy improvement of nearly 20%, topping the results of SVM before tuning its hyperparameters. Let's now do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7016574585635359"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_svm_gs = gs_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm_gs == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the hyperparameters were already optimal for the SVM, so Naive Bayes is performing better than SVM, at this stage. \n",
    "\n",
    "To improve the prediction accuracy, it would be interesting to integrate:\n",
    "- The title of the post, or \n",
    "- The graphic content of the meme \n",
    "as additional inputs for the models. \n",
    "\n",
    "Regarding the second option, the choice of the data was a bit unfortunate: The memes in this dataset are all made with the most popular meme templates of 2018 (aptly titled the same way). This means that the distribution of use of any given template across the subreddits will probably be relatively homogeneous, resulting in a low covariance. I regrettably didn't think about this implication until it was too late. On the other hand, the choice of public, prepared datasets with usable categories/topics was practically non-existent. With this being a clear limitation of this work, future efforts might explore a more graphically diverse dataset with a computer vision-enhanced classification method.\n",
    "\n",
    "Nevertheless, the first option seems a very viable way to improve prediction accuracy. This is what I will be exploring in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the title as input\n",
    "\n",
    "For this, we will use the title + transcription tokens made in the previous notebook. We just repeat the steps from before, but let's skip the training without hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(df[\"tokenized_merged_transc_title\"], df[\"category\"], random_state=42, test_size=0.2)\n",
    "X2_train_counts = count_vect.fit_transform(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255985267034991"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_2 = Pipeline([('vect', CountVectorizer()), # Counts words per sentence\n",
    "                     ('tfidf', TfidfTransformer()), # Avoids giving more weight to longer sentences and reduces weight of very common words\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "# text_clf_2 = text_clf_2.fit(X2_train, y2_train)\n",
    "gs_clf_2 = GridSearchCV(text_clf_2, parameters, n_jobs=-1)\n",
    "gs_clf_2 = gs_clf_2.fit(X2_train, y2_train)\n",
    "\n",
    "predicted_gs_2 = gs_clf_2.predict(X_test)\n",
    "np.mean(predicted_gs_2 == y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the Naive Bayes did not benefit from the additional information. Could it be that the post titles are not sufficiently correlated with the topic? Let's try with the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044198895027625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svm_2 = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', \n",
    "                                                   alpha=1e-3, random_state=42)),\n",
    "])\n",
    "\n",
    "gs_clf_svm_2 = GridSearchCV(text_clf_svm_2, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm_2 = gs_clf_svm_2.fit(X2_train, y2_train)\n",
    "\n",
    "predicted_svm_gs_2 = gs_clf_svm_2.predict(X_test)\n",
    "np.mean(predicted_svm_gs_2 == y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of logistic regression is ever so slightly higher, but probably not significant. It's safe to assume that the title of the post does not help the classification of the meme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
